{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Statistics lecture 2 Hands-on session : solutions notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the companion notebook to lecture 2 in the statistical course series, covering the following topics:\n",
    "1. Maximum likelihood basics\n",
    "2. Maximum likelihood for binned data\n",
    "3. Maximum likelihood properties\n",
    "4. Hypothesis testing basics\n",
    "\n",
    "First perform the usual imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Maximum likelihood basics\n",
    "\n",
    "Maximum likelihood is a powerful tool to *estimate* (=compute the value, in stats parlance) of unknown parameters. The principle is as follows: what we know is\n",
    "* The form of the PDF $P(x,\\theta)$, in terms of a random variable $x$ (the observable) and one or more unknown parameters $\\theta$.\n",
    "* A dataset, i.e. an observed value for the random variable\n",
    "The natural usage of the PDF is to draw values of $x$, for a given value of $\\theta$. What we need here is the reverse: we have the observed $x_{\\text{obs}}$, and we'd like to use it to infer the value $\\theta$.\n",
    "Maximum likelihood (ML) says that this can be done by maximizing the PDF, considered as a function of the $\\theta$ and with the random variable fixed to its observed value. This presentation of the PDF is called the *likelihood*, and it's really still just the PDF:\n",
    "$$\n",
    "L(\\theta) = P(x_{\\text{obs}}; \\theta)\n",
    "$$\n",
    "The ML estimator for $\\theta$ is then\n",
    "$$\n",
    "\\hat{\\theta} = \\text{arg max}_{\\theta} \\, L(\\theta)\n",
    "$$\n",
    "the value of $\\theta$ that maximizes $L$.\n",
    "\n",
    "We start with a simple example: a counting experiment where we observe $n=5$ with an expected background of $b=3$. The PDF is Poisson, with a parameter given as $s+b$, and we'd like to estimate $s$. The ML procedure can be performed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "b = 3\n",
    "\n",
    "s_values = np.arange(0, 5, 0.1) # We scan s from 0 to 5 in steps of 0.1\n",
    "\n",
    "# ==> Compute the likelihood for each value of s and plot the results; find the value of s (\"s-hat\") that maximizes the likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all goes well, you should get a maximum at $\\hat{s} = 2$ as one could naively expect.\n",
    "\n",
    "Note that while the position of the maximum is meaningful, the likelihood value isn't: likelihoods only need to be defined up to a multiplicative factor since we only use likelihood ratios in measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Maximum likelihood for binned data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things a bit more interesting, we move to the analysis of binned data. We take the same model as the one from the previous lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f2994bfe160>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nbins = 10\n",
    "x = np.linspace(0.5, nbins - 0.5, nbins)\n",
    "# The background follows a linear shape\n",
    "b_yields = np.array([ (1 - i/2/nbins) for i in range(0, nbins) ])\n",
    "b_yields *= b_yields/np.sum(b_yields)\n",
    "# The signal shape is a peak\n",
    "s_yields = np.zeros(nbins)\n",
    "s_yields[4:7] = [ 0.1, 0.8, 0.1 ]\n",
    "# Now generate some data\n",
    "s = 3\n",
    "b = 50\n",
    "s_and_b = s*s_yields + b*b_yields\n",
    "b_only = b*b_yields\n",
    "np.random.seed(0) # make sure we always generate the same data\n",
    "data = [ np.random.poisson(s*s_yield + b*b_yield) for s_yield, b_yield in zip(s_yields, b_yields) ]\n",
    "plt.bar(x, s_and_b, color='orange', yerr=np.sqrt(s_and_b), label='s+b')\n",
    "plt.bar(x, b_only, color='b', label='b only')\n",
    "plt.scatter(x, data, zorder=10, color='k', label='data')\n",
    "plt.xlabel('bin number')\n",
    "plt.ylabel('Total expected yield')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows the typical case of a binned analysis: we have 2 *templates*, i.e. reference shapes for signal and background, which are scaled by the yields $s$ and $b$. It should be obvious from the plot what the ML estimate $\\hat{s}$ is going to be: this just corresponds to the \"best fit\" of the templates to the data.\n",
    "\n",
    "If you're seeing the same example as on my machine, the content of bin 5 indicates something like $s = 2$, a positive signal but a bit smaller than the $s=3$ value for which we made the plot. Of course $s$ could also be negative, for instance if bin 5 was well below the background in this bin.\n",
    "\n",
    "We now check this by performing the ML estimation. A small change compared to the previous case is that instead of $L(s)$, we'll be using the quantity $\\lambda = -2 \\log L(s)$. The reason is as follows: first, the total PDF for all the bins together is a product of Poisson terms, one for each bin:\n",
    "$$\n",
    "L(s) = \\prod_{i=1}^{n_{\\text{bins}}} P(n_i, s f_{s,i} + b f_{b,i})\n",
    "$$\n",
    "Technically it's easier to deal with sums than products, and the log does this for us. As we'll see in a moment, this is also useful to deal with Gaussian PDFs, since in this case the log just extracts the squared term in the exponent. This is also the reason for the $-2$ term in the formula, since this term comes with a $-1/2$ factor.\n",
    "The only consequence in practical terms is that instead of $L(s)$ we compute $\\lambda(s)$, and that we now want to minimize instead of maximimizing. But the ML estimate $\\hat{s}$ remains the same, since the value of $s$ that maximizes $L(s)$ also minimizes $\\lambda(s)$.\n",
    "\n",
    "The next step is to find the ML estimate (best fit) $\\hat{s}$ using $\\lambda(s)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_values = np.arange(-2, 10, 0.1) # We scan s from -2 to 3 in steps of 0.1\n",
    "\n",
    "# ==> Compute \\lambda for each value of s, plot the result and find the best-fit value among the values tested;\n",
    "# ==> For later convenience, you may want to define a function lambda_s(s) that returns \\lambda as a function of s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all goes well, you should find a best-fit $\\hat{s}$ of about 2.3. Of course this value is a bit imprecise due to the step size of 0.1 in the scan, but we can do better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "# ==> Use the minimize_scalar function to find the minimum value. The syntax is minimize_scalar(func, (min, max)).x  (the .x returns the position of the minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find a best-fit value of about $\\hat{s} = 2.33$. Note than since $L(s)$ was defined only up to a multiplicative factor, $\\lambda(s)$ is defined up to an additive constant, so the value $\\lambda(\\hat{s})$ at the minimum isn't meaningful by itself. On the other hand, the difference between values of $\\lambda(s)$ at different $s$ is meaningful, and will be used later in hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Maximum-likelihood in the Gaussian case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen that the Poisson distribution can often be well approximated by a Gaussian (and other distributions as well, though the Central-limit theorem). If we write the expected number of events as\n",
    "$N_i = s f_{s,i} + b f_{b,i}$ then the likelihood is\n",
    "$$\n",
    "L(s) = \\prod_{i=1}^{n_{\\text{bins}}} P(n_i, N_i) \\propto \\prod_{i=1}^{n_{\\text{bins}}} \\exp\\left(-\\frac{1}{2} \\frac{(n_i - N_i)^2}{\\sigma_i^2} \\right)\n",
    "$$\n",
    "\n",
    "And then the -2 log likelihood is\n",
    "\n",
    "$$\n",
    "\\lambda(s) = -2 \\log L(s) = \\sum_{i=1}^{n_{\\text{bins}}}\\left(\\frac{n_i - N_i}{\\sigma_i}\\right)^2\n",
    "$$\n",
    "which is of course very familiar: it's just the usual $\\chi^2$. This is an important motivation for using $\\lambda(s)$ : it can be defined for any likelihood, but in the Gaussian case, it reduces to the $\\chi^2$. (Note that this is up to an additive constant, since the Gaussian PDF also has a multiplicative prefactor, but as discussed above the log-likelihood is anyway defined only up to an additive constant)\n",
    "\n",
    "The upshot is that for Gaussian cases, ML is the same as $\\chi^2$ minimization -- this is why we're using \"ML\" and \"best-fit\" interchangeably in this notebook. To illustrate this, we can repeat the example above in the Gaussian case, now estimating $\\hat{s}$ using the $\\chi^2$.\n",
    "\n",
    "For the standard (Pearson) $\\chi^2$, we take $\\sigma_i^2 = N_i$ : recall that for a Poisson distribution the variance is the same as the mean, so $\\sigma^2 = N$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==> Define a function to compute the chi2 between the model and the data as a function of s\n",
    "# ==> Plot the chi2(s) value and the lambda(s) values on the same plot to see if they are close to each other as expected. You will\n",
    "#     need to add a vertical offset to one of the curves to get good agreeement, which one can always do as discussed above. \n",
    "# ==> Use minimize_scalar to find the value of s which minimizes the chi2, and compare to what was found for lambda(s) above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all goes well, you should see that the $\\chi^2$ is quite close to the Poisson case above. The difference comes from the fact that the event counts are not quite large enough for the Poisson distributions to be quite Gaussian, and the residual non-Gaussianity leads to a small discrepency between the exact Poisson model and its $\\chi^2$ approximation.\n",
    "\n",
    "One can have an even less Gaussian situation by setting $s$ and $b$ to smaller values (say $s=2$ and $b=1$), so that the number of events is not large enough to trigger the Central-limit theorem. In this case one should get an asymmetric shape for the Poisson $\\lambda$ and sizable differences with the $\\chi^2$ approximation. \n",
    "\n",
    "**The rest of this section is optional -- feel free to skip if you are short on time (you should be about half way through the session at this point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==> (optional) Repeat the previous study for s=2 and b=1 to check for larger non-Gaussian effect.\n",
    "# ==> (optional) One can also repeat it again for larger counts such as b=1000, s=20 and check that the chi2 is now an excellent approximation to the Poisson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also define the alternate *Neyman* $\\chi^2$, where the uncertainty is taken from the observation rather than the expectation, so $\\sigma_i^2 = n_i$. This has some advantages, but typically agrees less well with the the Poisson limit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==> Repeat the previous study (going back to the original event yields), and add the Neyman chi2 curve to the plot with lambda(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hypothesis testing basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've focused on estimating parameters, but to get realistic results (including setting uncertainties on the best-fit values), we need an additional ingredient, *hypothesis testing*. The procedure for this is as follows:\n",
    "* Define the hypotheses to test. We need two of them: a baseline *null* hypothesis, and an *alternate* which is tested against it\n",
    "* Define the *test* to perform -- meaning the observables to use, and procedure to decide which hypothesis to accept.\n",
    "* Compute the result of the test for the observed data, in particular the *p-value* of the test, and decide whether to accept the null or the alternate hypothesis.\n",
    "\n",
    "To start with, we consider a simple Poisson counting process\n",
    "* Define two different signal hypotheses ($n=0$ and $n=5$)\n",
    "* Define the test in terms of the observed number of events: if it is less than some threshold, accept the null $n=0$, otherwise $n=5$.\n",
    "* As for any test, there are two ways to go wrong:\n",
    "   * Rejecting the null although it is true (false positive): the fraction of such cases is the Type-I error rate or p-value.\n",
    "   * Accepting the null although it is false (false negative) : : the fraction of such cases is the Type-II error rate, also (1 - Power)\n",
    "\n",
    "A more stringent test reduces the Type-I error rate, but at the expense of an increased Type-II rate and vice versa. This is illustrated by the *ROC curve* which shows the each error rate as a function of the other. The threshold for the test should be chosen depending on the desired balance between these error rates.\n",
    "\n",
    "For the study below, we consider a counting experiment and use the observed count $n$ as the discriminant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 10 # background of 10 events\n",
    "s_null = 0 # the null hypothesis: no signal\n",
    "s_alt  = 5 # the alternate hypothesis: \n",
    "\n",
    "ns = np.arange(1, 30, 1) # consider various observed values\n",
    "\n",
    "# ==> Compute and plot the PDFs for the two hypotheses. These are Poisson PDFs with parameters s_null + b and s_alt + b,\n",
    "#     and should be evaluated at the n values given in the array above.\n",
    "#     Reminder: the poisson PDF is given by scipy.stats.poisson.pmf(n, s+b)\n",
    "\n",
    "threshold1 = 12\n",
    "threshold2 = 16\n",
    "\n",
    "# ==> Compute the Type-I error probability (p-value) and the Type-II error probability for 2 tests: one with a threshold\n",
    "#     at n = 12 and the other at n=16\n",
    "#     Also optionally plot the results using the fill_between command of matplotlib, as done in lecture 1\n",
    "\n",
    "thresholds = np.arange(1, 30, 1)\n",
    "# ==> Compute the Type-I and Type-II error probabilities for each threshold in the list above, and plot the ROC curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The threshold for discovery in physics is often set at $5\\sigma$, which corresponds to a p-value of about $3 \\cdot 10^{-7}$. What threshold would one use to get so small a Type-I error rate ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==> Compute the value of the threshold on n which would give us a 5sigma discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find $n_{\\text{obs}} \\ge 29$. If we get such a large n, then we can indeed reject the null at with a p-value corresponding to a $5\\sigma$ threshold. This is the setting for a discovery: we have rejected the no-signal hypothesis in favor of an hypothesis with non-zero signal.\n",
    "\n",
    "However the power of the test is also very low, which means that even if the signal is really there, we are very unlikely to observe an event count of $n_{\\text{obs}} \\ge 29$. The reason for this is the fact that the alternate hypothesis corresponds to a relatively small event yield, on the scale of the measurement uncertainties, so it is very hard to find evidence for it.\n",
    "\n",
    "If we choose a hypothesis with a larger signal value, say $20$ signal events, then things are easier. The threshold for discovery is still 29 events (the p-value only depends on the null, not the alternative), but now the power is much larger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = 20\n",
    "# ==> Compute the power of the test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find that the power is about 50% -- i.e. and approximately 50% chance of discovery if there is indeed a signal of this size present."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
